"""A streamlit app to demonstrate PCA is conjunction with Echo State Networks - Linear Reg."""
from __future__ import annotations

import numpy as np
from sklearn.decomposition import PCA
import plotly.graph_objects as go

import streamlit as st
from streamlit_project.app_fragments import streamlit_utilities as utils

st.set_page_config("Linear Regression", page_icon="ğŸ“ˆ")

st.header("Linear Regression")

st.markdown(
    r"""
    In linear regression one tries to find a linear relationship between some explanatory 
    variables $\boldsymbol{r}$, and some dependent variables $\boldsymbol{y}$. 
    """
)

st.latex(r"""
\begin{aligned}
    \boldsymbol{r}&: \text{Explanatory variables of dimension } r_\text{dim} \\
    \boldsymbol{y}&: \text{Dependent variables of dimension } y_\text{dim}
\end{aligned}
""")

st.markdown(
    r"""
    The aim is to find the parameters of $y_\text{dim} \times r_\text{dim}$ matrix $W_\text{out}$, 
    and the $y_\text{dim}$ offset-vector $\boldsymbol{w}$, so that the relationship: 
    """
)

st.latex(r""" 
\begin{aligned}
    \boldsymbol{y}& = W_\text{out}\boldsymbol{r} + \boldsymbol{w} = W_\text{out}^*[\boldsymbol{r}, 
    1] = W_\text{out}^* \boldsymbol{r}^*
\end{aligned}
""")

st.markdown(
    r"""
    holds "as good as possible". We can incorporate the offset-vector $\boldsymbol{w}$ into the 
    matrix, by searching a  $r_\text{y} \times (r_\text{dim} + 1)$ matrix $W^*_\text{out}$, 
    where we also extend the $\boldsymbol{r}$ by one dimension with the constant value of 1. 
    Mathematically, one tries to find $W^*_\text{out}$, so that the squared difference, summed over 
    all data points $i$ is minimized: 
    """
)

st.latex(r""" 
\begin{aligned}
    \sum_{\text{sample: }i} \|\boldsymbol{y}_i& - W_\text{out}^* \boldsymbol{r}_i^*\|^2
\end{aligned}
""")


st.markdown(
    r"""
    $W^*_\text{out}$ can be calculated via: 
    """
)

st.latex(r"""
\begin{aligned}
    W_\text{out}^* = (R^T R)^{-1} R^T Y
\end{aligned}
""")

st.markdown(
    r"""
    Where $R$ and $Y$ are the matrices corresponding to the collection of data points of 
    $\boldsymbol{r}^*_i$ and $\boldsymbol{y}_i$ respectively. 
    """
)

st.markdown(
    r"""
    The above calculation assumes that the inverse of the 
    [moment matrix](https://en.wikipedia.org/wiki/Moment_matrix) is invertible. 
    """
)

st.latex(r"""
\begin{aligned}
    \text{Moment matrix} = R^T R \\
    (R^T R)_{j, k} = \sum_{\text{sample: }i} r^*_{i, j} r^*_{i, k} 
\end{aligned}
""")

st.markdown(
    r"""
    It turns out, $R^T R$ is only nicely invertible, if all features (dimensions of $r^*$) are
    not linear combinations of each other 
    (no [Multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)), e.g. if: 
    """
)

st.latex(r"""
\begin{aligned}
    (\hat{e}_k \cdot \boldsymbol{r}^*) = a + b (\hat{e}_j \cdot \boldsymbol{r}^*), k \neq j
\end{aligned}
""")

st.markdown(
    r"""
    holds for some dimensions $k, j$ in $\boldsymbol{r}$, there is multicollinearity and the the 
    moment matrix can not be inverted. The above equation shows a *perfect* multicollinearity. 
    Alternatively multicollinearity can be seen in a loose sense, where the there is only an 
    approximate linear relationship between two or more variables. 
    """
)

st.subheader("Issue of multicollinearity")

st.markdown(
    r"""
    **How multicollinearity can be detected:**:
    - **Condition of moment matrix** $R^TR$ (or of design matrix $R$?) ([Condition number](https://en.wikipedia.org/wiki/Condition_number)):
        - The conditioning of a matrix $A$ is defined as: $\text{cond}(A) = \|A\| \|A^{+}\|$, 
            where $A^+$ is the pseudo-inverse of $A$ and $\|\cdot\|$ can be any matrix norm. 
        - Links: [different matrix norms](https://en.wikipedia.org/wiki/Matrix_norm), 
            [NumPy function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.cond.html)
        - If the data shows multi-collinearity, the conditioning is very large! 
    - **Perturbing the data with noise and see how much the coefficients change**:
        - If the coefficients change a lot, the data is likely to contain multicollinearity. 
        - Note: Perturbing the data with noise already adds a sort of regularization! 
    - **Train on different subsets of the data, and see how the coefficients change**
    - **Rank of moment matrix:** 
        - When perfect multicollinearity exists, the rank of the moment matrix is less than the 
            dimension of the matrix. 
        - Links: [Rank of matrix](https://en.wikipedia.org/wiki/Rank_(linear_algebra)), 
            [NumPy rank function](https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html).
    """
)

st.markdown(
    r"""
    **How to tackle multicollineariy:** 
    - Mean center the predictor variables (?)
    - Standardize your independent variables
    - Shapley value? [Shapley value](https://en.wikipedia.org/wiki/Shapley_value)
    - Tikhonov regularization / ridge regression [Ridge regression](https://en.wikipedia.org/wiki/Ridge_regression)
    - Principal component regression [Principal_component_regression](https://en.wikipedia.org/wiki/Principal_component_regression) 
    """
)

st.markdown(
    r"""
    **Principal component regression (PCR):**
    - Instead of using the explanatory variables directly, the principal components are used as
        the regressors. 
    - If one uses only a subset of PCs, PCR is a kind of regularized linear regression. 
    - Major use case is: overcoming multicollinearity. 
    - Steps: 
        - Perform pca on the data matrix. (maybe only select a subset of components). 
        - Perform linear regression (or ridge regression?). 
        - Transform w_out back to get the w_out for the original features. 
    - Steps in detail: 
        - Datapreprocessing: Assume Y and X have been centered. 
        - Perform PCA on centered $X$. 
            - $X = U \Delta V^T$ is the singular value decomposition. With \Delta are the singular
                values (only on diagonal). $U$ and $V$ are both orthonormal sets of vectors, 
                denoting the left and right singular vectors of $X$. 
            - $V \Lambda V^T = V \Delta^2 V^T$ gives the spectral decomposition of $X^T X$. $\Lambda$ 
                contains the principal values on the diagonal. 
        - PCA Estimator: $W_\text{out, pca} = (R_\text{pca}^TR_\text{pca})^{-1} R_\text{pca}^TY$ 
            is the regressor for the pca transformed variables. $W_\text{out}$ for the original 
            variables can be estimated via $W_\text{out} = VW_\text{out, pca}$
    - Notes: 
        - If all components are considered, the PCA estimator is the same as the OLS estimate. 
    - Applications: 
        - Adressing multicollinearity: Exclude the columns that are multicollinear. 
        - Regularization effect: 
    """
)


def get_r_states(r_dim: int, n_samples: int, seed: int = 0) -> np.ndarray:
    """Get sample r_states.

    Args:
        r_dim: Nr of explanatory variables.
        n_samples: Nr of samples.
        seed: Seed for random number generator.

    Returns:
        The r_states of shape (n_samples, r_dim).
    """
    rng = np.random.default_rng(seed)
    r_states = rng.random((n_samples, r_dim))
    return r_states


def get_out_states(r_states: np.ndarray) -> np.ndarray:
    """Calculate the output states as a simple linear function of the input states.

    Args:
        r_states: The input states of shape (n_samples, r_dim).

    Returns:
        The output states of shape (n_samples, 1).
    """

    offset = 2
    out_states = np.sum(r_states, axis=-1)[:, np.newaxis] + offset
    return out_states


def get_multicol_r_states(r_states: np.ndarray,
                          nr_of_colinear_cols: int,
                          error_scale: float = 0.0,
                          seed: int = 0,
                          ) -> np.ndarray:
    """Take original_r_states and create a number of multi-colinear columns.

    Create the multi-colinearity by overwriting the ith column with the
    (r_dim - nr_of_colinear_cols + i)th column plus a normal error with scale "error_scale".

    Args:
        r_states: The original r_states of shape (n_samples, r_dim).
        nr_of_colinear_cols: The number of multi-colinear columns you wish to create. Has to be
                             smaller than r_dim - 1.
        error_scale: The scale of the gaussian error you add to the multi-colinear scale.
        seed: Seed for random number generator.

    Returns:
        The r_states with multi-colinearity introduced of shape (n_samples, r_dim).
    """

    if nr_of_colinear_cols >= r_states.shape[1]:
        raise ValueError("Nr of colinear columns has to be smaller than r_dim - 1. ")

    rng = np.random.default_rng(seed)
    error_values = rng.standard_normal(nr_of_colinear_cols) * error_scale
    r_states_m_col = r_states.copy()
    if nr_of_colinear_cols > 0:
        for i in range(nr_of_colinear_cols):
            r_states_m_col[:, i] = r_states_m_col[:, (r_dim - nr_of_colinear_cols) + i] + \
                                   error_values[i]
    return r_states_m_col


def get_r_gen_states(r_states: np.ndarray) -> np.ndarray:
    """Append a one to all r_states. """
    n_samples = r_states.shape[0]
    return np.hstack((r_states, np.ones(n_samples)[:, np.newaxis]))


def get_moment_matrix(r_gen_states: np.ndarray) -> np.ndarray:
    """Calculate the moment matrix R^T R."""
    return r_gen_states.T @ r_gen_states


def linear_regression(r_gen_states: np.ndarray, out_states: np.ndarray) -> np.ndarray:
    """Perform ordinary linear regression.

    Args:
        r_gen_states: Matrix of explanatory variables of shape (n_samples, r_gen_dim).
        out_states: Matrix of dependent variables of shape (n_samples, out_dim).

    Returns:
        The W_out matrix that fits out_states = W_out @ r_gen_states of shape (y_dim, r_gen_dim).
    """
    return np.linalg.solve(
        r_gen_states.T @ r_gen_states,
        r_gen_states.T @ out_states).T


def ridge_regression(r_gen_states: np.ndarray, out_states: np.ndarray,
                     reg_param: float = 1e-7) -> np.ndarray:
    """Perform ordinary linear regression.

    Args:
        r_gen_states: Matrix of explanatory variables of shape (n_samples, r_gen_dim).
        out_states: Matrix of dependent variables of shape (n_samples, out_dim).
        reg_param: Small regularization parameter for ill-conditioned / multi-collinear
                   r_gen_states.

    Returns:
        The W_out matrix that fits out_states = W_out @ r_gen_states of shape (y_dim, r_gen_dim).
    """
    return np.linalg.solve(
        r_gen_states.T @ r_gen_states + reg_param + np.eye(r_gen_states.shape[1]),
        r_gen_states.T @ out_states).T


def test_for_multicollinearity(moment_matrix: np.ndarray,
                               title: str | None = None) -> None:
    """Perform some tests to check for multicollinearity and show streamlit elements.

    Args:
        moment_matrix: 2D numpy array being the moment matrix R^TR of the data matrix R.
        title: Optional title.
    """
    rank = np.linalg.matrix_rank(moment_matrix)

    cond = np.linalg.cond(moment_matrix)
    if title is not None:
        st.markdown(f"**{title}**")
    cols = st.columns(2)
    with cols[0]:
        st.write("Condition: ")
        st.write(cond)
    with cols[1]:
        st.write("Rank: ")
        st.write(rank)


def train_on_subset(r_states: np.ndarray, out_states: np.ndarray, seed: int = 1,
                    n_samples_subset: int = 100, n_ens: int = 100):
    """Perform the linear regression on random subsets of r_states and out_states.

    Args:
        r_states: The original r_states of shape (n_samples, r_dim).
        out_states: Matrix of dependent variables of shape (n_samples, out_dim).
        seed: Seed for random number generator.
        n_samples_subset: Nr of samples per subset.
        n_ens: Nr of different subsets.

    Returns:
        W_out for each subset in the shape (n_ens, y_dim, r_gen_dim).
    """

    # Get parameters:
    n_samples, r_dim = r_states.shape
    if n_samples_subset > n_samples:
        raise ValueError("Nr of samples in subset can not be larger than total number of "
                         "samples. ")
    r_gen_dim = r_dim + 1
    out_dim = out_states.shape[1]

    # Configureate rng:
    rng = np.random.default_rng(seed)

    # Calculate w_out for each subset:
    w_out_subsets = np.zeros((n_ens, out_dim, r_gen_dim))
    total_list_of_indices = np.arange(n_samples)
    for i in range(n_ens):
        indices = rng.choice(total_list_of_indices, size=n_samples_subset, replace=False)
        r_states_subset = r_states[indices, :]
        r_gen_states_subset = get_r_gen_states(r_states_subset)
        out_states_subset = out_states[indices, :]
        w_out_subsets[i, :, :] = linear_regression(r_gen_states_subset, out_states_subset)

    return w_out_subsets


# def add_noise_and_do_reg(r_states: np.ndarray, out_states: np.ndarray,
#                          error_scale: float = 0.1, n_ens: int = 100,
#                          seed: int = 10):
#
#     # Get parameters:
#     n_samples, r_dim = r_states.shape
#     r_gen_dim = r_dim + 1
#     out_dim = out_states.shape[1]
#
#     # Create rng:
#     rng = np.random.default_rng(seed)
#
#     # add error to r_states and transform to r_gen:
#     r_gen_states_w_error = np.zeros((n_ens, n_samples, r_gen_dim))
#     for i in range(n_ens):
#         error = rng.standard_normal((n_samples, r_dim)) * error_scale
#         r_gen_states_w_error[i, :, :] = get_r_gen_states(r_states + error)
#
#     # For each realization do the fit and save w_out:
#     w_out_esn = np.zeros((n_ens, out_dim, r_gen_dim))
#     for i in range(n_ens):
#         r_gen_states = r_gen_states_w_error[i, :, :]
#         # out_states_w_error = out_states + rng.standard_normal(out_dim)
#         out_states_w_error = out_states
#         w_out_esn[i, :, :] = linear_regression(r_gen_states, out_states_w_error)
#
#     return w_out_esn
#     w_out_std = np.std(w_out_esn, axis=0)
#     return w_out_std
#     w_out_avg = np.mean(w_out_esn, axis=0)
#     return w_out_avg


st.header("Demonstration: ")


# NO MULTICOLINEARITY
st.markdown(
    r"""
    ##### No multicollinearity: 
    
    Get explanatory variables $\boldsymbol{r}$ and dependent variables $\boldsymbol{y}$. 
    Specify the number of sampels and the dimension of $\boldsymbol{r}$. 
    $R$ will be random generated by:
    
    `np.hstack((np.random((n_sampels, r_dim)), np.ones(n_samples)[:, np.newaxis]))`
    
    The dependent variables $y$ is one-dimensional and the sum of all expl. variables plus an 
    offset, i.e. 
    
    $\boldsymbol{y} = [\text{offset} + \sum_{\text{dim}: i} r_i, ]^T$. 
    
    The code is: `out_states = np.sum(r_states, axis=-1)[:, np.newaxis] + offset`
    """
)

# Get R states:
cols = st.columns(2)
with cols[0]:
    n_samples = st.number_input("n_samples", value=10000)
with cols[1]:
    r_dim = st.number_input("r_dim", value=10)

seed = 1
r_states = get_r_states(r_dim, n_samples, seed=seed)
r_gen_states = get_r_gen_states(r_states)
out_states = get_out_states(r_states)
moment_matrix = get_moment_matrix(r_gen_states)
# utils.st_line()

st.markdown(
    r"""
    Since all explanatory variables are created randomly, they don't show any correlation or 
    multicollinearity. 
    This can be seen by calculating the **rank** and **condition** of the moment matrix $R^TR$:
    """
)

test_for_multicollinearity(moment_matrix)
# utils.st_line()

# cond = np.linalg.cond(r_states)
# st.write("cond of r_states: ", cond)

st.markdown(
    r"""
    The rank of the moment matrix is $r_{dim} + 1$ (the $+1$ coming from the offset in the linear 
    fit.) Thus we can perform the linear fit without any problems. The output matrix is given as: 
    """
)

w_out_bias = linear_regression(r_gen_states, out_states)
st.markdown(r"$W^*_\text{out}$ (The last entry is the offset $\boldsymbol{w}$):")
st.write(w_out_bias)

st.markdown(
    r"""
    We can further investigate the multicollinearity, by training the system on random subsets 
    of the training data $X, Y$. The $W^*_\text{out}$ will vary between the subsets. The elements
    belonging to multicollinear columns will vary the most. Thus we can calculate the standard
    deviation of the $W^*_\text{out}$ elements over the different subsets. We use $100$ samples 
    per subset and a number of $100$ different subsets.  
    """
)

st.markdown(r"$\text{standard deviation of } W^*_\text{out}$:")

w_out_subset = train_on_subset(r_states, out_states, n_samples_subset=100, n_ens=100)
st.write(np.std(w_out_subset, axis=0, ))

st.markdown(
    r"""
    The standard deviation in $W^*_\text{out}$ is very low between the different subsets. 
    """
)

# Add noise
# st.markdown(
#     r"""
#     We can further investigate the multicollinearity, by adding noise to the design matrix $R$,
#     and observe how much the weights change during training. For an ensemble of $n_\text{ens} = 100$
#     $R$ and $Y$ pairs, with a noise scale (normal noise) of $0.01$, the **standard deviation**
#     within the w_out indices is calculated as:
#     """
# )
# st.markdown(r"$\text{standard deviation of } W^*_\text{out}$:")
# w_out_std = add_noise_and_do_reg(r_states, out_states, error_scale=0.01, n_ens=100)
# st.write(w_out_std)
# st.markdown(
#     r"""
#     The standard deviation in $W^*_\text{out}$ is very low.
#     """
# )

# WITH MULTICOLINEARITY
st.markdown(
    r"""
    ##### With multicollinearity: 
    
    Now we will introduce artificial multicollinearity into the trainings data, by overwriting a
    number variables as a linear function of other variables. 
    
    One can specify the number of variables that are supposed to be written as a linear combination
    of other variables. For imitating non-perfect multicolinearity one can add noise to the linear
    combination. 
    """
)

# Get multi-colinearity:
cols = st.columns(2)
with cols[0]:
    nr_of_colinear_cols = st.number_input("Nr of collinear columns",
                                          value=1,
                                          min_value=0,
                                          max_value=r_dim-1)
with cols[1]:
    error_scale = st.number_input("Error scale",
                                  value=0.01,
                                  step=0.01,
                                  min_value=0.0, format="%f")

r_states_m_col = get_multicol_r_states(r_states, nr_of_colinear_cols,
                                       error_scale, seed=seed)
out_states_m_col = get_out_states(r_states_m_col)
r_gen_states_m_col = get_r_gen_states(r_states_m_col)
moment_matrix_m_col = get_moment_matrix(r_gen_states_m_col)

test_for_multicollinearity(moment_matrix_m_col)

st.markdown(
    r"""
    We can see a huge condition and a rank of: $\text{rank} = r_\text{dim} - 
    \text{nr collinear var}$.
    """
)

w_out_m_col_bias = linear_regression(r_gen_states_m_col, out_states_m_col)
st.markdown(r"$W^*_\text{out}$ (The last entry is the offset $\boldsymbol{w}$):")
st.write(w_out_m_col_bias)

st.markdown(
    r"""
    We can already see that the $W^*_\text{out}$ values for the multicollinear columns are 
    different, than the values for the non-multicollinear columns. 
    
    As before by performing the linear regression on many subsets of $X$ and $Y$, we can 
    observe the standard deviation for each $W^*_\text{out}$ value: 
    """
)

st.markdown(r"$\text{standard deviation of } W^*_\text{out}$:")
w_out_subset_m_col = train_on_subset(r_states_m_col,
                                     out_states_m_col,
                                     n_samples_subset=100,
                                     n_ens=100)

st.write(np.std(w_out_subset_m_col, axis=0))

st.markdown(
    r"""
    We can clearly see, that the standard deviation is very large for entries corresponding to the 
    multicollinear columns. 
    """
)
